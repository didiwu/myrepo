+ Enable EPEL ceph repo and install ceph-deploy (controller/admin node)
# yum install -y yum-utils && sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && sudo yum install --nogpgcheck -y epel-release && sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && sudo rm /etc/yum.repos.d/dl.fedoraproject.org*
# cat /etc/yum.repos.d/ceph-deploy.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
# yum -y install ceph-deploy

+ Enable NTP (all nodes)
# yum install -y ntp
# systemctl enable ntpd
# systemctl start ntpd

+ Create ceph user and enable password-less login (all nodes)
# useradd ceph
# passwd ceph
# echo "Defaults:ceph !requiretty\nceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
# chmod 440 /etc/sudoers.d/ceph
# su - ceph
$ ssh-keygen
$ ssh-copy-id {mon/osd1/osd2}

+ Edit ceph .ssh/config file on controller/admin
$ cat .ssh/config
Host mon
   Hostname mon
   User ceph
Host osd1
   Hostname osd1
   User ceph
Host osd2
   Hostname osd2
   User ceph
   
+ Enable firewall ports
On mon node:
# firewall-cmd --add-port=6789/tcp --permanent
# firewall-cmd --reload

On osd nodes:
# firewall-cmd --add-port=6800-7100/tcp --permanent
# firewall-cmd --reload

+ Disable selinux (all nodes)
# cat /etc/selinux/config | grep SELINUX= | grep -v ^#
SELINUX=disabled
# setenforce 0

+ Enable yum-plugin-priorities
# yum install -y yum-plugin-priorities

+ Create ceph cluster (run this as ceph on controller/admin node)
$ mkdir my-clus
$ cd my-cluster
$ ceph-deploy new mon
$ cat ceph.conf
osd pool default size = 2
public network = 10.0.0.0/16
$ ceph-deploy install controller mon osd1 osd2
$ ceph-deploy mon create-initial

+ Prepare the journal disk on osd nodes
# parted -s /dev/vdb mklabel gpt
# parted -s /dev/vdb mkpart primary 0% 33%
# parted -s /dev/vdb mkpart primary 34% 66%
# parted -s /dev/vdb mkpart primary 67% 100%

+ Prepare the data disks on osd nodes
# parted -s /dev/vdc mklabel gpt
# parted -s /dev/vdc mkpart primary xfs 0% 100%
# mkfs.xfs /dev/vdc -f
# parted -s /dev/vdd mklabel gpt
# parted -s /dev/vdd mkpart primary xfs 0% 100%
# mkfs.xfs /dev/vdd -f
# parted -s /dev/vde mklabel gpt
# parted -s /dev/vde mkpart primary xfs 0% 100%
# mkfs.xfs /dev/vde -f

+ List and zap disks on osd nodes
$ ceph-deploy disk list osd1
$ ceph-deploy disk list osd2
$ ceph-deploy disk zap osd1:vdc
$ ceph-deploy disk zap osd1:vdd
$ ceph-deploy disk zap osd1:vde
$ ceph-deploy disk zap osd2:vdc
$ ceph-deploy disk zap osd2:vdd
$ ceph-deploy disk zap osd2:vde

Due to some ceph bug, we have to manually change the ownership of the journal disk on each OSD nodes and add the following udev rule:
# chown ceph /dev/vdb1
# chown ceph /dev/vdb2
# chown ceph /dev/vdb3
# cat /etc/udev/rules.d/10-local.rules
KERNEL=="vdb*", OWNER="ceph" GROUP="disk", MODE="0660"

+ Create OSDs on osd nodes
$ ceph-deploy osd create osd1:vdc:/dev/vdb1 osd1:vdd:/dev/vdb2 osd1:vde:/dev/vdb3
$ ceph-deploy osd create osd2:vdc:/dev/vdb1 osd2:vdd:/dev/vdb2 osd2:vde:/dev/vdb3

+ Copy the ceph.client.admin.keyring to /etc/ceph
$ sudo cp ceph.client.admin.keyring /etc/ceph
$ chmod 644 /etc/ceph/ceph.client.admin.keyring

+ Check the ceph status and increase pg_num and pgp_num as necessary
$ ceph status
$ ceph health
pg_num should be between (no_of_OSDs * 20) and (no_of_OSDs * 32)
$ ceph osd pool get rbd pg_num
$ ceph osd pool set rbd pg_num 192
$ ceph osd pool set rbd pgp_num 192
$ ceph -w

+ Addition steps (if required)
I've also found that the ceph mon service on the monitor node is not started up automatically. Do the following to enable auto start-up:
# systemctl enable ceph-mon.target

