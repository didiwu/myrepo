+ Enable ceph repo and install ceph-deploy (controller/admin node)
# cat /etc/yum.repos.d/ceph-deploy.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://ceph.com/rpm-hammer/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc
# yum -y install ceph-deploy

+ Enable NTP (all nodes)
# yum install -y ntp
# systemctl enable ntpd
# systemctl start ntpd

+ Create ceph user and enable password-less login (all nodes)
# useradd ceph
# passwd ceph
# echo "Defaults:ceph !requiretty\nceph ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/ceph
# chmod 440 /etc/sudoers.d/ceph
# su - ceph
$ ssh-keygen
$ ssh-copy-id {mon/osd1/osd2}

+ Edit ceph .ssh/config file on controller/admin
$ cat .ssh/config
Host mon
   Hostname mon
   User ceph
Host osd1
   Hostname osd1
   User ceph
Host osd2
   Hostname osd2
   User ceph
   
+ Enable firewall ports
On mon node:
# firewall-cmd --add-port=6789/tcp --permanent
# firewall-cmd --reload

On osd nodes:
# firewall-cmd --add-port=6800-7100/tcp --permanent
# firewall-cmd --reload

+ Disable selinux (all nodes)
# cat /etc/selinux/config | grep SELINUX= | grep -v ^#
SELINUX=disabled
# setenforce 0

+ Enable yum-plugin-priorities
# yum install -y yum-plugin-priorities

+ Create ceph cluster (run this as ceph on controller/admin node)
$ mkdir my-clus
$ cd my-cluster
$ ceph-deploy new mon
$ cat ceph.conf
osd pool default size = 2
public network = 10.0.0.0/16
$ ceph-deploy install controller mon osd1 osd2
$ ceph-deploy mon create-initial

+ Prepare the journal disk on osd nodes
# parted -s /dev/vdb mklabel gpt
# parted -s /dev/vdb mkpart primary 0% 33%
# parted -s /dev/vdb mkpart primary 34% 66%
# parted -s /dev/vdb mkpart primary 67% 100%

+ List and zap disks on osd nodes
$ ceph-deploy disk list osd1
$ ceph-deploy disk list osd2
$ ceph-deploy disk zap osd1:vdc
$ ceph-deploy disk zap osd1:vdd
$ ceph-deploy disk zap osd1:vde
$ ceph-deploy disk zap osd2:vdc
$ ceph-deploy disk zap osd2:vdd
$ ceph-deploy disk zap osd2:vde

Due to some ceph bug, we have to manually change the ownership of the journal disk on each OSD nodes:
# chown ceph /dev/vdb1
# chown ceph /dev/vdb2
# chown ceph /dev/vdb3

+ Create OSDs on osd nodes
$ ceph-deploy osd create osd1:vdc:/dev/vdb1 osd1:vdd:/dev/vdb2 osd1:vde:/dev/vdb3
$ ceph-deploy osd create osd2:vdc:/dev/vdb1 osd2:vdd:/dev/vdb2 osd2:vde:/dev/vdb3

+ Copy the ceph.client.admin.keyring to /etc/ceph
$ sudo cp ceph.client.admin.keyring /etc/ceph
$ chmod 644 /etc/ceph/ceph.client.admin.keyring

