+ Disable Network Manager

# systemctl disable NetworkManager
# systemctl stop NetworkManager

+ Assign static IPs and hostname

# cat /etc/hostname
storage1.localdomain
# cat /etc/sysconfig/network
HOSTNAME=storage1
GATEWAY=172.168.0.2
# cat /etc/sysconfig/network-scripts/ifcfg-eth0
TYPE=Ethernet
BOOTPROTO=static
NAME=eth0
DEVICE=eth0
ONBOOT=yes
IPADDR=172.168.0.21
NETMASK=255.255.0.0
NM_CONTROLLED=no
# systemctl restart network

+ Enable NTP
# yum install -y ntp
# systemctl enable ntpd
# systemctl start ntpd

+ Disable firewalld on gluster nodes
# systemctl disable firewalld
# systemctl stop firewalld

+ Install GlusterFS

wget -P /etc/yum.repos.d/ http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-epel.repo
rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum install glusterfs glusterfs-fuse glusterfs-server
systemctl start glusterd.service

+ Prepare disks where the gluster volumes will reside and create gluster volumes

pvcreate /dev/vdb1
vgcreate -n vg_data /dev/vdb1
vgcreate vg_data /dev/vdb1
lvcreate -l +100%FREE -n lv_data vg_data
mkfs.xfs /dev/mapper/vg_data-lv_dat

gluster peer probe storage1
gluster peer probe storage2
gluster peer status
gluster pool list
gluster volume status

mkdir /gluster

gluster volume create openstack storage1:/gluster storage2:/gluster force

gluster volume start openstack
gluster volume status
gluster volume info


+ Create stack user and sudo access 

+ Install RDO Openstack (as stack user from contorller node)

$ sudo yum install -y centos-release-openstack-mitaka
$ sudo yum update -y
$ sudo yum install -y openstack-packstack
$ packstack --install-hosts=10.0.0.10,10.0.0.11,10.0.0.12 --cinder-backend=gluster --cinder-gluster-mounts=storage1:/cinder

+ Remove existing default routes and networks

+ Create new public and private networks and routes

# cat /etc/sysconfig/network-scripts/ifcfg-br-ex
DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROTO=static
IPADDR=192.168.0.10
NETMASK=255.255.0.0
GATEWAY=192.168.0.2
DNS1=192.168.0.1
ONBOOT=yes

# cat /etc/sysconfig/network-scripts/ifcfg-eth2
DEVICE=eth2
TYPE=OVSPort
DEVICETYPE=ovs
OVS_BRIDGE=br-ex
ONBOOT=yes

# systemctl restart network

# cat /etc/neutron/plugins/ml2/ml2_conf.ini | grep type_drivers
type_drivers = vxlan, flat

# systemctl restart neutron-server.service

# . keystonerc_admin

# neutron net-create external_network --provider:network_type flat --provider:physical_network extnet  --router:external

# neutron subnet-create --name public_subnet --enable_dhcp=False --allocation-pool=start=192.168.0.100,end=192.168.0.200 --gateway=192.168.0.2 external_network 192.168.0.0/16

# neutron router-create router1

# neutron router-gateway-set router1 external_network

# neutron net-create private_network

# neutron subnet-create --name private_subnet private_network 172.16.100.0/24

# neutron router-interface-add router1 private_subnet


+ Configure glance image to use GlusterFS

On one of the storage node:
# gluster vol set glance storage.owner-uid 161
# gluster vol set glance storage.owner-gid 161

On OpenStack controller node:
# systemctl stop openstack-glance-api.service
# systemctl stop openstack-glance-registry.service
# mv /var/lib/glance/images /var/lib/glance/images.old
# mkdir /var/lib/glance/images
# wget -P /etc/yum.repos.d/ http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-epel.repo
# yum install glusterfs glusterfs-fuse
# cat /etc/fstab
storage1:/glance        /var/lib/glance/images  glusterfs       defaults,_netdev 0 0
# mount -a
# chown glance:glance /var/lib/glance/images
# chmod 750 /var/lib/glance/images
# systemctl start openstack-glance-api.service
# systemctl start openstack-glance-registry.service

Download an image and upload it to glance:
# glance image-create --property name=centos7 --property --visibility=public --container-format bare --disk-format qcow2 < CentOS-7-x86_64-GenericCloud.qcow2

+ Live migration
# nova-manage service list
# nova list
# nova live-migration <VM_ID> <Hypervisor_hostname>
